apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: naglfar-imdb-benching-
spec:
  entrypoint: starter
  onExit: delete-ns
  arguments:
    parameters:
      - name: queries # split to 4 parts
#        value: "20a.sql 20b.sql 20c.sql 21a.sql 21b.sql 21c.sql 23a.sql 23b.sql 23c.sql 24a.sql 24b.sql 27a.sql 27b.sql 27c.sql 29a.sql 29b.sql 29c.sql"
        value: "2a.sql 2b.sql 2c.sql 2d.sql 32a.sql 32b.sql 3a.sql 3b.sql 3c.sql"
#        value: "10a.sql 11a.sql 11b.sql 11c.sql 15a.sql 15b.sql 15c.sql 15d.sql 16a.sql 16b.sql 16c.sql 16d.sql 17a.sql 17b.sql 17c.sql 17d.sql 17e.sql 17f.sql 19a.sql 19b.sql 19c.sql 19d.sql"
#        value: "5a.sql 5b.sql 5c.sql 6a.sql 6b.sql 6c.sql 6d.sql 6e.sql 6f.sql 7a.sql 7b.sql 7c.sql 8a.sql 8b.sql 8c.sql 8d.sql 9a.sql 9b.sql 9c.sql 9d.sql"
      ######################################### current part
      - name: release                           # release version: nightly
        value: nightly
      - name: tidb-url
        value: "http://fileserver.pingcap.net/download/builds/pingcap/tidb/f99982b0c594c14ed3c92f62329e3dfd68e884d5/centos7/tidb-server.tar.gz"
      - name: tikv-url
        value: "http://fileserver.pingcap.net/download/builds/pingcap/tikv/d71c78ef7c74b65e429ce24bd91389d06eb8ec0b/centos7/tikv-server.tar.gz"
      - name: pd-url
        value: "http://fileserver.pingcap.net/download/builds/pingcap/pd/8ff72c9eda8f358a4f9a5f8d7787dfb13511b5aa/centos7/pd-server.tar.gz"
      - name: toolset-tag                       # we use br to restore data, so you need fill in the toolset version
        value: tidb-5.0-rc                      # option: latest | tidb-5.0-rc . latest for master
      ######################################### baseline part
      - name: baseline-release
        value: nightly
      - name: baseline-tidb-url                 # if need to patch baseline tidb binary version
        value: ""
      - name: baseline-tikv-url
        value: ""
      - name: baseline-pd-url
        value: ""
      - name: baseline-toolset-tag              # we use br to restore data, so you need fill in the toolset version
        value: latest                           # option: latest | tidb-5.0-rc. latest for master
  templates:
    - name: starter
      steps:
        - - name: create-ns
            template: create-ns
        - - name: request-cluster
            template: request-cluster
        - - name: create-cluster
            template: create-cluster
            arguments:
              parameters: [ { name: release-branch, value: "{{workflow.parameters.release}}" },
                            { name: tidb-url, value: "{{workflow.parameters.tidb-url}}" },
                            { name: tikv-url, value: "{{workflow.parameters.tikv-url}}" },
                            { name: pd-url, value: "{{workflow.parameters.pd-url}}" } ]
        - - name: importer-logs-1
            template: importer-logs
            arguments:
              parameters: [ { name: toolset-tag,   value: "{{workflow.parameters.toolset-tag}}" },
                            { name: release-branch, value: "{{workflow.parameters.release}}" } ]
        - - name: benching-logs-1
            template: benching-logs
        - - name: uninstall-cluster
            template: uninstall-cluster
        - - name: create-cluster-baseline
            template: create-cluster
            arguments:
              parameters: [ { name: release-branch, value: "{{workflow.parameters.baseline-release}}" },
                            { name: tidb-url, value: "{{workflow.parameters.baseline-tidb-url}}" },
                            { name: tikv-url, value: "{{workflow.parameters.baseline-tikv-url}}" },
                            { name: pd-url, value: "{{workflow.parameters.baseline-pd-url}}" } ]
        - - name: importer-logs-2
            template: importer-logs
            arguments:
              parameters: [ { name: toolset-tag,   value: "{{workflow.parameters.baseline-toolset-tag}}" },
                            { name: release-branch, value: "{{workflow.parameters.baseline-release}}" } ]
        - - name: benching-logs-2
            template: benching-logs
    - name: importer-logs
      inputs:
        parameters:
          - name: toolset-tag
          - name: release-branch
      steps:
        - - name: importing
            template: importing
            arguments:
              parameters: [ { name: toolset-tag,    value: "{{inputs.parameters.toolset-tag}}" },
                            { name: release-branch, value: "{{inputs.parameters.release-branch}}" }]
          - name: importing-logs
            template: logs
            arguments:
              parameters: [ { name: workload_name, value: "imdb-importing" } ]
        - - name: importing-deletion
            template: workload-deletion
            arguments:
              parameters: [ { name: workload_name, value: "imdb-importing" } ]
    - name: benching-logs
      steps:
        - - name: benching
            template: benching
          - name: benching-logs
            template: logs
            arguments:
              parameters: [ { name: workload_name, value: "imdb-benching" } ]
        - - name: importing-deletion
            template: workload-deletion
            arguments:
              parameters: [ { name: workload_name, value: "imdb-benching" } ]
    - name: logs
      inputs:
        parameters:
          - name: workload_name
      container:
        name: logs
        image: 'argoproj/argoexec:latest'
        imagePullPolicy: IfNotPresent
        command:
          - sh
          - '-c'
          - |
            while true
            do
                state=`kubectl get tw {{inputs.parameters.workload_name}} -n {{workflow.name}} -ojsonpath='{.status.state}' || echo pending`
                if [ "running" = "$state" ]; then
                    break
                fi
                if [ "succeeded" = "$state" ]; then
                    break
                fi
                if [ "failed" = "$state" ]; then
                    break
                fi
                echo "workload isn't already now. Wait 10s..."
                sleep 10
            done
            curl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/PingCAP-QE/Naglfar/master/scripts/kubectl-naglfar-installer.sh | sh
            export PATH=$PATH:/root/.Naglfar/bin
            naglfar logs {{inputs.parameters.workload_name}} -n {{workflow.name}} --follow
    - name: request-cluster
      resource:
        action: create
        successCondition: status.state = ready
        manifest: |
          apiVersion: naglfar.pingcap.com/v1
          kind: TestResourceRequest
          metadata:
            name: tidb-cluster
            namespace: {{workflow.name}}
          spec:
            machines:
              - name: m1
                exclusive: true
              - name: m2
                exclusive: true
            items:
              - name: tidb
                spec:
                  memory: 64GB
                  cores: 32
                  machine: m1
              - name: pd
                spec:
                  memory: 8GB
                  cores: 2
                  machine: m1
              - name: tikv-1
                spec:
                  memory: 64GB
                  cores: 32
                  disks:
                    disk1:
                      kind: nvme
                      mountPath: /disk1
                  machine: m2
              - name: workload
                spec:
                  memory: 8GB
                  cores: 4
                  machine: m2
    - name: create-cluster
      inputs:
        parameters:
          - name: release-branch
          - name: tidb-url
          - name: tikv-url
          - name: pd-url
      resource:
        action: create
        successCondition: status.state = ready
        manifest: |
          apiVersion: naglfar.pingcap.com/v1
          kind: TestClusterTopology
          metadata:
            name: tidb-cluster
            namespace: {{workflow.name}}
          spec:
            resourceRequest: tidb-cluster
            tidbCluster:
              serverConfigs:
                tidb: |-
                tikv: |-
                  #rocksdb.defaultcf.block-cache-size: "16GB"
                  #rocksdb.writecf.block-cache-size: "9GB"
                  #rocksdb.lockcf.block-cache-size: "1GB"
                pd: |-
              version:
                version: {{inputs.parameters.release-branch}}
                tidbDownloadURL: {{inputs.parameters.tidb-url}}
                tikvDownloadURL: {{inputs.parameters.tikv-url}}
                pdDownloadURL: {{inputs.parameters.pd-url}}
              control: tidb
              tikv:
                - host: tikv-1
                  port: 20160
                  statusPort: 20180
                  deployDir: /disk1/deploy/tikv-20160
                  dataDir: /disk1/data/tikv-20160
                  logDir: /disk1/deploy/tikv-20160/log
              tidb:
                - host: tidb
                  deployDir: /disk1/deploy/tidb-4000
              pd:
                - host: pd
                  deployDir: /disk1/deploy/pd-2379
                  dataDir: /disk1/data/pd-2379
                  logDir: /disk1/deploy/pd-2379/log
              monitor:
                - host: tidb
                  deployDir: /disk1/deploy/prometheus-8249
                  dataDir: /disk1/deploy/prometheus-8249/data
              grafana:
                - host: tidb
                  deployDir: /disk1/deploy/grafana-3000
    - name: uninstall-cluster
      resource:
        action: delete
        manifest: |
          apiVersion: naglfar.pingcap.com/v1
          kind: TestClusterTopology
          metadata:
            name: tidb-cluster
            namespace: {{workflow.name}}
    - name: importing
      inputs:
        parameters:
          - name: toolset-tag
          - name: release-branch
      resource:
        action: create
        successCondition: status.state = succeeded
        failureCondition: status.state = failed
        manifest: |
          apiVersion: naglfar.pingcap.com/v1
          kind: TestWorkload
          metadata:
            name: imdb-importing
            namespace: {{workflow.name}}
          spec:
            clusterTopologies:
              - name: tidb-cluster
                aliasName: cluster
            workloads:
              - name: imdb-importing
                dockerContainer:
                  resourceRequest:
                    name: tidb-cluster
                    node: workload
                  image: "hub.pingcap.net/mahjonp/bench-toolset:{{inputs.parameters.toolset-tag}}"
                  imagePullPolicy: IfNotPresent
                  command:
                    - /bin/bash
                    - -c
                    - |
                      #!/bin/bash
                      set -ex
                      export AWS_ACCESS_KEY_ID=minioadmin AWS_SECRET_ACCESS_KEY=minioadmin
                      tidb=`echo $cluster_tidb0 | awk -F ":" '{print $1}'`
                      pd=`echo $cluster_pd0 | awk -F ":" '{print $1}'`
                      release_branch="{{inputs.parameters.release-branch}}"

                      if [[ ${release_branch:0:4} == "v5.0" ]]; then
                        br restore db --db=imdb --pd $pd:2379 --storage s3://benchmark/imdb \
                          --s3.endpoint http://172.16.5.109:9000 --send-credentials-to-tikv=true
                      elif [[ ${release_branch} == "nightly" ]]; then
                        br restore db --db=imdb --pd $pd:2379 --storage s3://benchmark/imdb \
                          --s3.endpoint http://172.16.5.109:9000 --send-credentials-to-tikv=true
                      else
                        echo "illegal release_branch: $release_branch"
                        exit 1
                      fi
    - name: benching
      resource:
        action: create
        successCondition: status.state = succeeded
        failureCondition: status.state = failed
        manifest: |
          apiVersion: naglfar.pingcap.com/v1
          kind: TestWorkload
          metadata:
            name: imdb-benching
            namespace: {{workflow.name}}
          spec:
            clusterTopologies:
              - name: tidb-cluster
                aliasName: cluster
            workloads:
              - name: imdb-benching
                dockerContainer:
                  resourceRequest:
                    name: tidb-cluster
                    node: workload
                  image: "hub.pingcap.net/mahjonp/bench-toolset"
                  imagePullPolicy: IfNotPresent
                  command:
                    - /bin/bash
                    - -c
                    - |
                      set -ex
                      curl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/chaos-mesh/horoscope/master/install.sh | sh
                      git clone https://github.com/chaos-mesh/horoscope.git
                      cd horoscope/benchmark/job
                      mv queries queries.back && mkdir queries

                      sqls=( {{workflow.parameters.queries}} )
                      for file in "${sqls[@]}"; do
                        cp queries.back/$file queries/$file
                      done
                      tidb=`echo $cluster_tidb0 | awk -F ":" '{print $1}'`
                      /root/.horo/bin/horo -d "root@tcp($tidb:4000)/imdb?multiStatements=true" -w . init
                      /root/.horo/bin/horo bench --round 4
    - name: workload-deletion
      inputs:
        parameters:
          - name: workload_name
      resource:
        action: delete
        manifest: |
          apiVersion: naglfar.pingcap.com/v1
          kind: TestWorkload
          metadata:
            name: {{inputs.parameters.workload_name}}
            namespace: {{workflow.name}}
    - name: create-ns
      resource:
        action: create
        successCondition: status.phase = Active
        manifest: |
          apiVersion: v1
          kind: Namespace
          metadata:
            name: {{workflow.name}}
    - name: delete-ns
      resource:
        action: delete
        manifest: |
          apiVersion: v1
          kind: Namespace
          metadata:
            name: {{workflow.name}}
